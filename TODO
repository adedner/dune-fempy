BUGS:
test/testd2.py: assertion failure!

-------------------------------------------------------------------------

BUG: signature (e.g. of ufl local function not correctly set), str(expr) can give different results
     first:  uflSignature: ['[(({ A | A_{i_{56}} = -1 * ([sin(3.141592653589793 * x[0]) * sin(3.141592653589793 * x[1])])[i_{56}] }) + solution) : (({ A | A_{i_{56}} = -1 * ([sin(3.141592653589793 * x[0]) * sin(3.141592653589793 * x[1])])[i_{56}] }) + solution), (grad(({ A | A_{i_{56}} = -1 * ([sin(3.141592653589793 * x[0]) * sin(3.141592653589793 * x[1])])[i_{56}] }) + solution)) : (grad(({ A | A_{i_{56}} = -1 * ([sin(3.141592653589793 * x[0]) * sin(3.141592653589793 * x[1])])[i_{56}] }) + solution))]']
     second: uflSignature: ['[(({ A | A_{i_{1490}} = -1 * ([sin(3.141592653589793 * x[0]) * sin(3.141592653589793 * x[1])])[i_{1490}] }) + solution) : (({ A | A_{i_{1490}} = -1 * ([sin(3.141592653589793 * x[0]) * sin(3.141592653589793 * x[1])])[i_{1490}] }) + solution), (grad(({ A | A_{i_{1490}} = -1 * ([sin(3.141592653589793 * x[0]) * sin(3.141592653589793 * x[1])])[i_{1490}] }) + solution)) : (grad(({ A | A_{i_{1490}} = -1 * ([sin(3.141592653589793 * x[0]) * sin(3.141592653589793 * x[1])])[i_{1490}] }) + solution))]']


BUG: model signature does not take name of constants into account -
    therefore demo/heat.py an notebooks/laplace_coefficient.py leads to
    problem when executed after each other


General:
1) Far more safty checks and docu
2) Automated testing also of bindings in other modules so some CMake
   command like 'dune_test_grid_bindings' or 'dune_test_bindings(grid)'
3) Installation issues
4) we seem to be loosing between 5-10% run time (s dune-fem-dg and
   Forchheimer test). Where is the overhead?


dune-fempy/dune-fem:
1) use petsc (scipy) matrix from outside to construct linear operator (for
   assembly) without copy (same for discrete functions see issue #49)
4)     binding for dune-fem ode solvers, construction of new ode solvers from numpy arrays,
       (other project: ode solver model generation (ufl+butcher -> list of coupled models))
6)     checkpointing (can we use pickle)
10)    problem with symmetry in integrands (see dgbug.py)
11)    evaluate exact meaning of v['+'] or v['-'] in integrands form
       (see email to Martin and also issue #48)
12)    improve boundary id provider mechanism:
       - how to pass in boundary ids in with 'dictionary' construction, i.e., without dgf
       - make the id provider replacable in the model classes, i.e., use some
         callback mechanism possibly with caching during construction

dune-python:
1) parallel builder: issues are small and slow disks on parallel machines,
   NFS issues on clusters, also docker issue with 'dune' != user
   - http builder (sort of deamon process doing the build also good for docker)
   - use different dune-py for each process
   - if I'm on a cluster and write into shared home then NFS slows down
     things BUT if I use a local storage then the cache needs to be rebuild
     on each new machine I lock into
2) extend builder to not require full dune modules for 'algorithms' or even
   simple classes (see issue #41)
3) build smaller execution units for parallel or on demand build (shorten
   grid generation for example). Is it possible to speed things up using
   parallel make?
5) docker handling: volume always needs to be removed for different
   projects due to modularity
6) use free standing function in __init__ (should work using decltype to
    get DuneType and factory method in constructor as in
       https://pybind11.readthedocs.io/en/stable/advanced/classes.html?highlight=init#custom-constructors
5) make it possible to compile generate code for multiple classes in
   parallel - example [space,df]. Could either be in one module
   (space.DiscreteFunction) or using parallel make
7) provide mechanism to set CXXFLAGS pass during code generation
8) make hgrid.globalRefine(factor=0.3) always reduce h by 0.5, i.e., use refineStepsForHalf?
9)  extract dimworld,dimgrid from dgf file
10) change name of refinement for ALU - it is not about conforming/non
    conforming but about bisection and ...
17) Add a way to distiguish between local and global coordinates, i.e.,
    add a "LocalFieldVectorClass" derived from FieldVector but not adding
    that base in the pybind11 export statement. Then export all method
    taking/returning local coordinate using this new type. On C++ side
    everything works as before due to the base class but on python side the
    cast is not available.
18) check http://www.rafekettler.com/magicmethods.html#representations
    so that __double__ works for FieldVector<double,1>. Also the other
    cast should work

Other issues in dune-fempy:
#23, #30, #34, #35, #39, #46, #47, #48


from firedrake:
>>>> n = FacetNormal(mesh)
>>>> un = 0.5*(dot(u0, n) + abs(dot(u0, n)))

bcval = Constant(0.0)
bc = DirichletBC(V, bcval, 1)
bcval.assign(sin(2*pi*5*t))


a = inner(grad(u), grad(v))*dx
L = -v*ds(3) + v*ds(4)
nullspace = VectorSpaceBasis(constant=True)
u = Function(V)
solve(a == L, u, nullspace=nullspace)
exact = Function(V)
exact.interpolate(Expression('x[1] - 0.5'))
print sqrt(assemble((u - exact)*(u - exact)*dx))

Moving Meshes:
http://firedrakeproject.org/mesh-coordinates.html

UFL Expresion using TSFC:
http://firedrakeproject.org/interpolation.html#c-string-expressions


Further Schemes etc:
- vem scheme (needs special grid part for polygon)
- nvdg scheeme (needs A(u,grad u) term for hessian multiplication in model
- complete ns scheme to evaluate full rhs model
- add fv scheme (how to do numerical fluxes)?
- chemical flow problem?
