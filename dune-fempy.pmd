\documentclass{ansarticle}

\definecolor{hrefcolor}{rgb}{0.0,0.0,0.8}
\newcommand{\linkcolor}{hrefcolor}
\hypersetup{pdfstartview=FitR}
\hypersetup{pdftitle={The DUNE-Fempy Module},
            pdfauthor={L. Connellan, A. Dedner, M. Nolte},
            breaklinks=true,
            linkbordercolor=\linkcolor,
            urlbordercolor=\linkcolor,
            citebordercolor=\linkcolor,
            runbordercolor=\linkcolor,
            menubordercolor=\linkcolor,
            filebordercolor=\linkcolor,
            runcolor=\linkcolor
            baseurl={https://gitlab.dune-project.org/extensions/dune-alugrid}
}
% make links underlined in acrobat reader
\hypersetup{pdfborderstyle={/S/U/W 1}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}

%<*pre>
\usepackage{a4wide}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{anslistings}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage{comment}

\newcommand{\dune}[1][]{\textsc{Dune}\ifx&#1&\else\textsc{-{#1}}\fi\xspace}
\newcommand{\alugrid}{\textsc{ALUGrid}\xspace}
\newcommand{\ug}{\textsc{UG}\xspace}
\newcommand{\sionlib}{\textsc{SIONlib}\xspace}
\newcommand{\alberta}{\textsc{ALBERTA}\xspace}
\newcommand{\yaspgrid}{\textsc{YaspGrid}\xspace}

\newcommand{\matplotlib}{\pyth{Matplotlib}\xspace}
\newcommand{\quadpy}{\pyth{Quadpy}\xspace}
\newcommand{\numpy}{\pyth{Numpy}\xspace}
\newcommand{\scipy}{\pyth{Scipy}\xspace}
\newcommand{\paraview}{Paraview\xspace}
\newcommand{\vtk}{VTK\xspace}
\newcommand{\cmake}{cmake\xspace}
\newcommand{\pybind}{Pybind11\xspace}
\newcommand{\code}[1]{\lstinline[basicstyle=\small\sffamily]{#1} }
\newcommand{\file}[1]{\code{#1}}

\newcommand{\Atodo}[1]{\todo[inline]{{\bf\color{green}Andreas:} #1}}
\newcommand{\Ltodo}[1]{\todo[inline]{{\bf\color{blue}Lloyd:} #1}}
\newcommand{\Mtodo}[1]{\todo[inline]{{\bf\color{green}Martin:} #1}}
\newcommand{\Rtodo}[1]{\todo[inline]{{\bf\color{red}Robert:} #1}}

\newcommand{\autocite}{\cite}
%</pre>
\newcommand{\pweavecaption}{}
\newcommand{\pweavelabel}{}
\lstnewenvironment{pweavecode}[1][]{\lstset{escapechar=`,moredelim=[il][]{\#tex},style=pythonstyle, title={\codetitlestyle{Python code }},belowcaptionskip=\belowtitleskip, caption=\pweavecaption,label=\pweavelabel}}{}
\lstnewenvironment{pweaveout}[1][]{\lstset{style=gencodestyle, title={\codetitlestyle{Output}}, belowcaptionskip=\belowtitleskip}}{}

\begin{document}

\title{The \dune[Fempy] Module}
\author[1]{Lloyd Connellan}% \thanks{l.connellan@warwick.ac.uk}}
\author[2]{Andreas Dedner}% \thanks{a.s.dedner@warwick.ac.uk}}
\author[3]{Martin Nolte}%\thanks{nolte@mathematik.uni-freiburg.de}}
\affil[1]{University of Warwick, UK}
\affil[2]{University of Warwick, UK}
\affil[3]{University of Freiburg, Germany}
\runningtitle{The \dune[Fempy] Module}
\runningauthor{Connellan, Dedner, Nolte}

\providecommand{\keywords}[1]{\textbf{Keywords: } #1}

\date{November, 2017}

\maketitle

```python, label="Intro", echo=False
# <markdowncell>
# Python code from the Dune-FemPy paper
# ======================================
# This Jupyter notebook contains the code examples shown in the
# Dune-Python paper.
# The sectioning reflects that of the paper and, while some introductory
# comments are given, we refer to it for the details.
#
# __Note__: Some of the code cells depend on previous ones and cannot be
# executed in arbitrary order.
# <codecell>
import time, numpy, math, sys
try:
    import petsc4py
    petsc4py.init(sys.argv)
    from petsc4py import PETSc
except:
    petsc4py = False
import dune.plotting
dune.plotting.block = False
```

\begin{abstract}
In this paper we present the new \dune[Fempy] module...

\noindent
\keywords{..., \dune}
\end{abstract}

% \tableofcontents

\section{Introduction}

%<*intro>

Among the different numerical methods for solving partial differential
equations, finite element methods are one of the most popular. They have been used
for a broad range of engineering and scientific problems, with the first
computational applications originating as early as \cite{Turner}. Over the
years there has been an extensive amount of literature analysing FEMs
in general and their uses (see e.g. \cite{Khoei} and \cite{Babuska}).

Just as the development of the theory of FEMs has progressed over the years,
the landscape of FEM software itself has undergone much change.
As a multidisciplinary method involving many different techniques, the
scope for which direction to develop features is very high. Even
within the realm of standard FEMs there exist a multitude of different
options available. For instance with regards to types of finite
elements, if one considers \textit{conforming} finite elements
(i.e. where $V_h \subset V$) then one has possibilities such as the
well-known Lagrange element, the $H(div)$ conforming
Brezzi-Douglas-Marini element used for instance for
the elastic stress tensor, the $H(curl)$ conforming N\'{e}d\'{e}lec element used in
electromagnetism, and so on. Then, provided one uses an appropriate penalty
method, one can further expand this to have nonconforming elements such as the $H^2$
Hermite (cubic) elements, the Morley (quadratic) elements used for fourth order
problems, and the $H^1$ Crouzeix-Raviart element used in Stokes
flow\footnote{For a more complete list of types of elements, see
\cite{FEniCS2}}. In addition one can consider the mesh itself; one can
have structured grids that are more computationally efficient or
unstructured ones that allow for more flexibility. Furthermore one could
have different shapes such as squares, triangles, cubes, pyramids,
hexahedrons and so on. This is all without going into more complex
forms of FEM such as hp-FEM, spectral element methods and extended
finite element methods (XFEM).

All in all this diversity of choice has lead to the situation of numerous
competing packages that offer slightly different
flavours of FEM. One preventative measure to this has been the
development of large modular software libraries that offer many
optional extensions in one place, thus forgoing the need to install
different packages for different problems. Such examples include \dune
(\cite{Dune-Grid}), deal.II (\cite{dealII}), FreeFem++
(\cite{FreeFem}) and Elmer (\cite{Elmer}). These large packages are
typically written in languages such as C++ and Fortran that are
efficient for large-scale computations.

In recent years however there has been a trend towards packages that
favour usability, especially given the rise in popularity of Python as
a programming language. Such packages ideally look to lower the
learning curve for new developers and non-computer focused
researchers, without compromising the functionality and efficiency of
traditional packages. An additional benefit to using Python is the
facilitating of rapid prototyping, which allows one to quickly
construct new models and test their viability without having to write
an intricate program. In particular FEniCS (\cite{FEniCS}) and Firedrake
(\cite{Firedrake}) are examples of this kind of software. Integral to this approach has
been using a simplified interface attached to a back end with lower
level code, which is typically achieved by an automatic code
generation tool like SWIG or Cython.

One large component of this approach is the use of Unified Form Language
(UFL) (\cite{UFL}), a domain-specific language (DSL) which
allows one to write variational equations directly. For instance
for Poisson's equation we have the following simple code.
\renewcommand{\pweavecaption}{Poisson's equation in UFL}
\renewcommand{\pweavelabel}{lst:poisson}
\begin{pweavecode}
a = inner(grad(u), grad(v))*dx
b = inner(f, v)*dx
\end{pweavecode}
Nonetheless there are potential weaknesses in the code generation
approach. In particular this generated code is not
suited to direct editing, so should a binding not exist for a feature
on the Python side, editing these generated files to add the feature
on the C++ side is not an option. Furthermore, user interactibility with the
C++ interface is not prioritized, which means porting code over to C++
for efficiency reasons or the writing of additional features are not
viable.

In this paper we introduce \dune[Fempy], a \dune module that is an
extension to \dune[Python] (\cite{Dune-Py}) specifically aimed at
adding high-level FEM features based on the \dune[Fem] module
(\cite{Dune-Fem}) to \dune. The aim of both of these packages is to bring the
usability and speedier writing of code to \dune and its large array of existing modules,
whilst preserving the features available to a C++ developer. In
particular the structure and functionality is designed to be analogous
in many ways to \dune code, making it less difficult to port code to
C++ if necessary. Additionally, attempts to increase usability have been
made, such as library caching to reduce the runtime of repeated computations, and
integration with modern C++11/C++14 via pybind11 (see
\cite{Pybind11}) to interface between C++ and Python.

%</intro>

The structure of the paper is as follows. In section
\ref{sec:fem-in-fempy} we introduce the interface for a simple
FEM step-by-step. In sections \ref{sec:solve} and \ref{sec:header} we
look more in-depth at this example and some of the design flexibility
available. In sections \ref{sec:crystal} and \ref{sec:mcf} % and \ref{sec:battery}
we look at additional features in the context of more complex examples.
Finally we discuss the comparison to C++ code in section
\ref{sec:mcf-comparison}.

\Rtodo{Include example of ellipticmodel.hh}
\Atodo{do we want to add something on parallelism}
\Rtodo{Parallelism only maybe for one example and only if it's already working.}
\Atodo{which other features are missing? Should we include an example on
non variational problems or bounding box dg? Perhaps we could show r
adaptivity (also shows off the moving grid capability) and the use of
dune-femnv?  twophaseflow example? p-adaptivity? Uzawa examples?}
\Atodo{describe new 'dune.get(category=None,entry=None)' function listing all available
       implementations of a given category, e.g., grid,space etc.}
\Atodo{shuld we 'import' all spaces,function etc. into 'dune.fem' to allow
       'from dune.fem import *'}

%<*tag>

\section{Finite Element Methods in \dune[Fempy]} \label{sec:fem-in-fempy}
```python, label="fem-in-fempy", echo=False
# <markdowncell>
# ## Finite Element Methods in \dune[Fempy]
# In this section we introduce the basic components of a finite
# element method when applied to the Forchheimer problem.
# <codecell>
```

When designing any software package, a natural challenge that arises
is trying to make the user interface as simple and easy to use as possible.
At the same time however, we also want to create an interface that retains
all the functionality we need.

In the context of finite element methods, this leads to the question
of what the minimal functional structure for a FEM looks like. In
order to try to address this question, we will first outline from a
mathematical standpoint the general structure we have in mind for a
FEM.

To begin with, the original problem we typically want to apply a finite
element method to is a continuous PDE in some
infinite-dimensional space $V$. First let $\Omega \subset \mathbb{R}^d$ be
a polygonal domain for our problem. We then choose a conforming
finite element space $V_h = \{\varphi_h : \Omega \to \mathbb{R}^r \}
\subset V$, where dim $V_h = N$. This involves choosing a basis for
$V_h$, which can vary depending on the problem, but typically involves
piecewise polynomial functions.

Next the variational (or weak) form of the equation is defined. For
the purpose of illustration, let us assume to start with we have a
parabolic PDE of the following general form.
\begin{equation*}
  \begin{array}{ll}
  \partial_t u + L[u] = 0, & \mbox{in } \Omega \times [0, T], \\
  u(x, 0) = u^0(x), & \mbox{in } \Omega, \\
  D\nabla u \cdot \textbf{n} = g(x), & \mbox{on } \Gamma \times [0, T],
  \end{array}
\label{eqn:parabolic}
\end{equation*}
where the elliptic operator $L$ is defined as
\begin{equation}
  L[u] := -\nabla \cdot D(x, t, u, \nabla u) + m(x, t, u, \nabla u),
\label{eqn:elliptic}
\end{equation}
and where $u^0$ and $g$ are the initial and boundary conditions and
$\textbf{n}$ is the outward pointing normal. We note that we are
only considering Neumann boundary conditions at first for simplicity,
although Dirichlet boundary conditions are also a possibility.

To obtain the discrete form, we begin by applying a simple time stepping
scheme, starting from
the initial data $u^0$ we define a sequence $u^n$ satisfying
\begin{equation}
  \frac{u^{n+1}-u^n}{\Delta t} + L_I[u^{n+1}] + L_E[u^n] = 0~,
\label{eqn:rotheStrong}
\end{equation}
where $\Delta t$ is a time step and
$L_I$ and $L_E$ are the implicit and explicit parts of $L$,
defined as
\begin{gather}
  L_I[u] = -\nabla\cdot D_1(x, t^{n+1}, u, \nabla u) + m_1(x, t^{n+1}, u, \nabla u), \quad \\
  L_E[u] = -\nabla\cdot D_2(x, t^n, u, \nabla u) + m_2(x, t^n, u, \nabla u),
\label{eqn:im-ex}
\end{gather}
and $D_1 + D_2 = D$, $m_1 + m_2 = m$. The variational formulation of
\eqref{eqn:rotheStrong} is
\begin{equation}
  \int_\Omega \frac{u^{n+1}-u^n}{\Delta t}\varphi +
  \int_\Omega D_1(x, t^{n+1}, u^{n+1}, \nabla u^{n+1}\cdot\nabla\varphi +
        m_1(x, t^{n+1}, u^{n+1}, \nabla u^{n+1})\varphi
  +\int_\Omega D_2(x, t^n, u^n, \nabla u^n)\cdot\nabla\varphi +
        m_2(x, t^n u^n, \nabla u^n)\varphi
  - \int_{\partial\Omega} \frac{1}{2}(g(t^{n+1},x)+g(t^n,x))\varphi = 0
\label{eqn:rothe}
\end{equation}
The final step is to solve this system for $u^{n+1} \in V_h$. This also
involves potential variation in terms of the solver used and possible
nonlinearity of the problem. We also note that this is a simple scheme
for demonstration, and more complex examples involving higher order
schemes or nonconforming spaces can be easily implemented along the
same lines.

With this general form in mind, in \dune[Fempy] we have designed the
structure to take as similar a style as possible, which results in the
following breakdown of parts.

\begin{itemize}
  \item \ref{sec:grids} Grid. The computational domain $\Omega$ the problem is set in.
  \item \ref{sec:functions} Grid functions. Functions defined on the grid
  used for example for the solution $u_h$ but also for the exact solution.
  \item \ref{sec:spaces} Space. The finite element space $V_h$ and type of basis functions.
  \item \ref{sec:schemes} Scheme. The weak form of the equation, its
  boundary conditions, and method for solving.
  \item \ref{sec:solving} Solving. The actual solving process and data
  output.
\end{itemize}

We note that there exist even further simplifications that can be made
in terms of this design choice; for instance a FEM could be
distilled to simply choosing a weak form (an operator) and a grid,
and having all other things set to sensible defaults. Additionally the code
itself used to represent these methods could be simplified to a large
degree depending on the aim of the software.

Ultimately as a FEM package aimed more at extensibility and for
researchers who are willing to commit to some degree of
programming, we have opted for more complexity in some cases at the
expense of this simplicity. In general this is quite a nuanced design
decision that must be made without a clear \textit{right} answer.

With that in mind, for the remainder of this section we shall
demonstrate in more detail how each of these concepts are implemented
in the context of a worked example, the FEM applied to
the Forchheimer equation: a scalar,
nonlinear parabolic equation derived in \cite{Kieu}.
\begin{equation} \label{eqn:forch-roche}
  \int_{\Omega} \frac{u^{n+1}-u^n}{\Delta t} \varphi
  + \frac{1}{2}K(\nabla u^{n+1}) \nabla u^{n+1} \cdot \nabla \varphi \
  + \frac{1}{2}K(\nabla u^n) \nabla u^n \cdot \nabla \varphi v \diff x
  - \int_{\Omega} \frac{1}{2}(f(x,t^n)+f(x,t^n+\Delta t) \varphi \diff x
  - \int_{\partial \Omega} \frac{1}{2}(g(x,t^n)+g(x,t^n+\Delta t)) v \diff s.
\end{equation}
where the diffusion tensor is given by
\begin{equation}
  K(\nabla u) = \frac{2}{1+\sqrt{1+4|\nabla u|}}
\end{equation}
and $f=f(x,t)$ is some forcing term.
This corresponds to taking $D_I=D_E=\frac{1}{2}K(\nabla u) \nabla u$
and $m_I=\frac{1}{2}f(x,t),m_E=\frac{1}{2}f(x,t)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Grids} \label{sec:grids}

The first aspect of FEMs that we consider is probably the most
fundamental aspect, that of the grid (or mesh). Naturally, before looking at the
equations we want to solve themselves, we must look at the
computational domain and how we want to discretize it. Ideally
numerical software looking to emulate FEMs should be able to construct both simple
triangulated 2D domains and more complex surfaces and meshes.

For now we will look at a simple example. Let us suppose we
have a domain of the following form.
\begin{equation*}
  \Omega = \{ (x,y) \in \mathbb{R}^2 : 0 \leq x \leq 1, 0 \leq y \leq 1\}.
\label{eqn:domain}
\end{equation*}
In creating a computational grid for this domain, it will be necessary to specify
the following things.
\begin{enumerate}
  \item The shape of the domain (a square) and its vertices.
  \item The number of elements.
  \item The type of elements (e.g. square elements or triangles).
\end{enumerate}
With these points in mind, we implement the grid in the following way
in \dune[Fempy] and plot the result in figure \ref{fig:grid}.
\renewcommand{\pweavecaption}{Creating and plotting two simple
rectangular grids}
\renewcommand{\pweavelabel}{lst:grid}
```python, label="grid", Fig=True, caption="Plot of a 2D grid for two different levels of refinement", width="0.475\\textwidth"
from dune.grid import structuredGrid
grid = structuredGrid([0, 0], [1, 1], [4, 4])
grid.plot()
grid.hierarchicalGrid.globalRefine(1)
grid.plot()
grid.hierarchicalGrid.globalRefine(-1)  # revert grid refinement
```
Here we create a simple square domain by specifying two opposite
corners $(0,0)$ and $(1,1)$, and the number
of elements in each direction $(4,4)$. We then refine the grid and
plot the results, before coarsening it again. We note that this is a
simplified example and in general grids in \dune[Fempy] can
additionally be constructed via a dictionary containing vertex and
element information, gmsh files or \textit{dune grid format} (dgf)
files when more complexity is required, which is demonstrated in the
\dune[python] paper \cite{Dune-Py}. A list of more complicated
grids and other modules is given in \ref{sec:modules}.

Conceptually it is worth stating that from a design standpoint, assumptions
could potentially be made to cut down on the complexity needed.
For instance in situations where the exact details are
not necessary, a basic square grid could simply be made with
\pyth{grid = square()}. However such a design comes at the cost of it
being unclear how to make small modifications.

\subsection{Grid Functions} \label{sec:functions}

Having defined the computational domain and function space, we look
towards functions that we may need to define, e.g. for containing the
solution. In particular we want to be able to store what initial
values it can take, its value at the previous time step and so on.

Let us begin by just considering functions $f\colon\Omega\to\R^r$
e.g. the initial condition $u^0$.
In \dune[Fempy], we use the Unified Form Language (UFL) (\cite{UFL}) to define
equations, which is essentially a human-readable way of writing functions
and variational forms.
To this end, we must begin by defining a variable to describe the spatial
variable $x$:
\renewcommand{\pweavecaption}{Creating an \pyth{x} variable in UFL}
\renewcommand{\pweavelabel}{lst:forch-uflx}
```python, label = 'forch-uflx'
from ufl import SpatialCoordinate, triangle
x = SpatialCoordinate(triangle)
```
Here we create \pyth{x} as a 2D spatial coordinate from UFL.
So now for initial condition
$u^0 = \frac{1}{2}(x_0^2 + x_1^2) - \frac{1}{3}(x_0^3 - x_1^3) + 1$,
we would have the following code:
\renewcommand{\pweavecaption}{Creating a grid function using UFL}
\renewcommand{\pweavelabel}{lst:forch-initial}
```python, label = 'forch-initial'
from ufl import as_vector
initial = as_vector( [1/2*(x[0]**2+x[1]**2) - 1/3*(x[0]**3 - x[1]**3) + 1] )
```
Now this function can be used in a variety of ways. Let us first
show how we would compute the mass of the initial function.
We do this using the function \pyth{integrate} as follows:
\renewcommand{\pweavecaption}{Integrating the initial data}
\renewcommand{\pweavelabel}{lst:forch-integrate}
```python, label="fem2", echo=False
# <markdowncell>
# We can easily easily integrate grid function
# <codecell>
```
```python, label = 'forch-integrate'
from dune.fem.function import integrate
mass = integrate(grid, initial, order=5)
print(mass)
```
The arguments to the function \pyth{integrate} are the grid describing the
domain of integration, a UFL expression describing the integrand and
the parameter \pyth{order} giving the order of the quadrature to use.
Note that at the time of writing \dune[Fempy] only operates with vectored
valued expressions and forms.

\Atodo{Here it would be nice to be able to provide a quadrature instead of only an order}

We can also plot functions fairly easily. The two main ways to do
this in \dune[Fempy] are either a quick plot in \pyth{matplotlib}
(see \cite{Matplotlib}), or writing to a
VTK file for use in Paraview (see \cite{Paraview}), which we do
below, resulting in figure \ref{fig:forch-plot}.
\renewcommand{\pweavecaption}{Plotting a function using two different methods}
\renewcommand{\pweavelabel}{lst:forch-plot}
```python, label="fem2", echo=False
# <markdowncell>
# and plot them using matplotlib or write a vtk file for postprocessing
# <codecell>
```
```python, label = 'forch-plot', fig=True, caption="The matplotlib plot of the initial function", width="0.475\\textwidth"
from dune.fem.plotting import plotPointData as plot
plot(initial, grid=grid)
grid.writeVTK('initial', pointdata={'initial': initial})
```
For the vtk output the function needs to be assigned a name, which is given
by the key argument of the dictionary passed as the \pyth{pointdata}
argument.

Note that so far we have simply used the UFL expressions, e.g., \pyth{initial}
directly, i.e., without using any approximation and in fact the functions
we have used so far, e.g., for integration and plotting do not require the
UFL expression to be approximated. In some cases it can be necessary to
convert the expression into a \emph{grid function}, i.e., a function that
can be localized to one element of the grid and evaluated there in local
coordinates. To this end we can use the function \pyth{uflFunction}
and for example compute a simple approximation of the mass
```python, label = 'uflfunction'
from dune.fem.function import uflFunction
initial_gf = uflFunction(grid, name="ufl", order=1, ufl=initial)
mass = 0
for element in grid.elements:
  mass += initial_gf(element,[0.5,0.5]) * element.geometry.volume
print(mass)
```
Since with \dune[Python] the complete grid interface is available in the
Python script this simplifies rapid prototyping of new algorithms also
using grid functions given by UFL expressions which can later also be used
in C++ code.

Finally to obtain a discrete
function we need to define finite dimensional spaces which we will do in
the next section.

\subsection{Spaces} \label{sec:spaces}

The next key part of a FEM after constructing the grid is defining the
kind of finite element we want to use, and by extension their space. In
particular this is important because the order and type of elements used can
dictate the solvability and the efficiency of the method.

Let us consider a simple case of Lagrange elements. Since we have a 2D
domain with a quadrilateral mesh, we consider shape functions that are 1 on
each separate node, and 0 on the others. For orders 2 and 3, the shape
functions would be quadratic and cubic polynomials respectively (as
shown in figure \ref{fig:lagrange-elements}).
\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.4\linewidth]{figures/lagrange-ele2.jpg}
  \caption{Second order element}
  \label{fig:lagrange2}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.4\linewidth]{figures/lagrange-ele3.png}
  \caption{Third order element}
  \label{fig:lagrange3}
\end{subfigure}
\caption{Node maps of two Lagrange reference elements}
\label{fig:lagrange-elements}
\end{figure}
The creation of such a Lagrange space in \dune[Fempy] is done by the
following code.
\renewcommand{\pweavecaption}{Creating a Lagrange space with
polynomial basis functions}
\renewcommand{\pweavelabel}{lst:space}
```python, label="fem1", echo=False
# <markdowncell>
# Setting up a discrete function space and some grid function
# <codecell>
```
```python, label = 'space'
from dune.fem.space import lagrange
space = lagrange(grid, dimrange=1, order=2)
```
We note that the above space is called with two default
arguments and two keyword arguments.
\begin{itemize}
  \item \pyth{'lagrange'} indicates that we will use a space with
  Lagrange basis functions.
  \item \pyth{grid} passes in the grid we constructed previously.
  \item \pyth{dimrange=1} (optional) sets the dimension of the range
  space to $r=1$
  \item \pyth{order=2} (optional) sets the order of the finite
  elements to 2 (1 is the default).
\end{itemize}
Of particular note is that the first argument corresponds to a \dune
discrete space realization that can come from anywhere within a \dune
installation, provided Python bindings are created for it. For instance we could use a
discontinuous Galerkin space with orthonormal basis functions instead
by using \pyth{'dgonb'}.

Now we can create discrete grid function by using the default interpolation
into the finite element space given by the nodal variables on the space.
The method \pyth{interpolate} can be used for that:
\renewcommand{\pweavecaption}{}
\renewcommand{\pweavelabel}{lst:forch-uh}
```python, label="fem3", echo=False
# <markdowncell>
# So far we used grid functions defined globally. An important subclass of
# grid functions are discrete functions over a given discrete function space.
# The easiest way to construct such functions is to use the interpolate
# method on the discrete function space:
# <codecell>
```
```python, label = 'forch-uh'
u_h = space.interpolate(initial, name='u_h')
```
So we have created a discrete function \pyth{u_h} over the discrete
finite element space which we will use to contain the solution $u^{n+1}$
to our PDE and used an interpolation over the space to assign its initial value by
interpolation of the UFL expression \pyth{initial}. The name is used later for plotting
purposes for example in the vtk output. Discrete function are a special
form of grid functions so have the same properties shown in the previous
section:
\renewcommand{\pweavecaption}{Plotting discrete function using \matplotlib}
\renewcommand{\pweavelabel}{lst:forch-plot-uh}
```python, label="fem2", echo=False
# <markdowncell>
# and plot them using matplotlib or write a vtk file for postprocessing
# <codecell>
```
```python, label = 'forch-plot-uh', fig=True, caption="The matplotlib plot of the initial function", width="0.475\\textwidth"
plot(u_h, grid=grid)
```

To define the weak formulation given by (\ref{eqn:forch-roche}) we need two
discrete function, one to store the next time step ($u^{n+1}$) and a second one
$u^{n}$ containing the approximation of the present time step. We use
\pyth{u_h} to store the former and construct a copy of \pyth{u_h} to store
the later:

\renewcommand{\pweavecaption}{Copying a discrete function}
\renewcommand{\pweavelabel}{lst:forch-u_h_n}
```python, label = 'forch-u_h_n'
u_h_n = u_h.copy(name="previous")
```
Given two discrete function we can use \pyth{u_h_n.assign(u_h)} to assign
one to the other.

\Atodo{Move the describtion of the error computation to here, i.e., show
the interpolation error for the initial conditions?}

\subsection{Schemes} \label{sec:schemes}
\Atodo{Explain that JIT compilation happens behind the scheme and show
       \pyth{uflFunction} to make the process explicit}
\Atodo{Show how to evaluate grid functions?}

\Atodo{Note that grid function are ufl coefficients so can be directly used}

In \dune[Fempy], we define schemes as the object combining a
discretization of a given weak form, its boundary conditions and the method used
to approximate the inverse e.g. the iterative linear solver to use.
Specifically, for an operator $L: V_h \to V_h^{*}$, schemes have two
main methods:
\begin{enumerate}
\item Apply the operator (\pyth{__call__}). That is to calculate $w_h = L[v_h]$ given some
$v_h \in V_h$. If Dirichlet boundary conditions are part of the problem
then $w_h = v_h - g$ on the Dirichlet boundary, where $g$ is the boundary
function.
\item Solve the PDE (\pyth{solve}). That is to compute the solution $u_h$ to $L[u_h] = v_h$ given some $v_h \in
V_h^{*}$, by using a solve method. By default $v_h = 0$.
\end{enumerate}
\noindent \textit{Remark.} In the case where only the operator application is
required/possible (e.g. when $L:V \to W \neq V$ , an \pyth{operator}
object can be constructed instead of a scheme which does not implement a solve method.

\noindent \textit{Remark.} By default the \pyth{solve} method solves the
residual problem, e.g., $L[u_h]=0$ in the domain (and $u_h-g=0$ on the
Dirichlet boundary if present). The \pyth{solve} method can take an
optional argument \pyth{rhs} in which case $L[u_h]=$\pyth{rhs} will be
solved.

\Atodo{should we also mention the 'constraint' methods for handling Dirichlet bcs}

Recall the parabolic equation (\ref{eqn:forch-roche}), which we will focus
on in the following example. To begin with, it is
necessary to define the variables that are used in the equation.
\renewcommand{\pweavecaption}{Setting up UFL variables to be used}
\renewcommand{\pweavelabel}{lst:forch-variables}
```python, label="fem4", echo=False
# <markdowncell>
# Now we can set up our PDE model
```
Recall that the semi discrete problem was
\begin{equation}
  \int_{\Omega} \frac{1}{\Delta t} (u^{n+1} - u^n) v \
  + \frac{1}{2}K(\nabla u^{n+1}) \nabla u^{n+1} \cdot \nabla v \
  + \frac{1}{2}K(\nabla u^n) \nabla u^n \cdot \nabla v \diff x
  = \int_{\Omega} f v \diff x + \int_{\partial \Omega} g v \diff s.
  \tag{\ref{eqn:forch-roche}}
\end{equation}
and we force the system by defining $f$ so that
\begin{equation}
  u(x,t) = e^{-2t}\left(\frac{1}{2}(x_0^2 + x_1^2) - \frac{1}{3}(x_0^3 - x_1^3)\right) + 1
         = e^{-2t}\left(u^0(x)-1) + 1
\end{equation}
becomes the exact solution.
We can use \pyth{initial} to define this using some algebra, and we
write a lambda function that takes \pyth{t} as argument.
\renewcommand{\pweavecaption}{The exact solution}
\renewcommand{\pweavelabel}{lst:forch-exact}
```python, label = 'forch-exact'
from ufl import exp
exact = lambda t: as_vector([exp(-2*t)*(initial[0]  - 1) + 1])
```
Finally $g$ is defined by assigning the
value to the boundary term using the exact solution.
```python, label = 'forch-variables'
from ufl import TestFunction, TrialFunction
from dune.ufl import NamedConstant
u = TrialFunction(space)
v = TestFunction(space)
dt = NamedConstant(space, "dt")    # time step
t  = NamedConstant(space, "t")     # current time
```
The trial function $u$ and the test function $v$ are defined on the
same \pyth{space} as before. Additionally $\Delta t$ and $t$ are
defined as \pyth{NamedConstant}, which is simply a UFL \pyth{Constant}
variable that can be given a name so it can be more easily modified
later on.

Now for the equation (\ref{eqn:forch-roche}) itself,
resulting in an implementation of the following form.
\renewcommand{\pweavecaption}{Implementing the weak form}
\renewcommand{\pweavelabel}{lst:forch-eqn-impl}
```python, label = 'forch-eqn-impl'
from ufl import dx, grad, div, inner, sqrt
abs_du = lambda u: sqrt(inner(grad(u), grad(u)))
K = lambda u: 2/(1 + sqrt(1 + 4*abs_du(u)))
a = ( inner((u - u_h_n)/dt, v) \
    + 0.5*inner(K(u)*grad(u), grad(v)) \
    + 0.5*inner(K(u_h_n)*grad(u_h_n), grad(v)) ) * dx
```
To define the forcing $f$, we put the exact
solution into the strong form of the equation
(i.e. $f = u_t - \nabla \cdot (K(\nabla u) \cdot \nabla u$).
We also add in Neumann boundary
conditions by substituting the exact solution into the boundary term (obtained
after differentiation by parts).
\renewcommand{\pweavecaption}{Setting up the right hand side}
\renewcommand{\pweavelabel}{lst:forch-rhs}
```python, label = 'forch-rhs'
from ufl import dot, FacetNormal, ds
f = lambda s: -2*exp(-2*s)*(initial[0] - 1) - div( K(exact(s))*grad(exact(s)[0]) )
g = lambda s: K(exact(s))*grad(exact(s)[0])
n = FacetNormal(space)
b = 0.5*(f(t)+f(t+dt))*v[0]*dx + 0.5*dot(g(t)+g(t+dt),n)*v[0]*ds
```
Finally, having defined the weak form and right hand side, we can
now set up a scheme object which we can use to solve the PDE.
\renewcommand{\pweavecaption}{Creating an $H^{1}$ scheme}
\renewcommand{\pweavelabel}{lst:forch-scheme}
```python, label="fem5", echo=False
# <markdowncell>
# With the model described as a ufl form, we can construct a scheme class
# that provides the solve method which we can use to evolve the solution from
# one time step to the next:
# <codecell>
```
```python, label = 'forch-scheme'
import dune.create as create
scheme = create.scheme("galerkin", a == b, solver='cg')
```
The above function generates code to assemble and solve
a Galerkin method, with the space and equation passed in.
As before
there exist other such premade \dune schemes for different
types of problems (see \ref{sec:scheme-list}).
Note that by default a Newton method is used to solve the resulting system
and either a preconditioned Krylov solver or a direct solver is used to
solve the linear system in each step. The type of Krylov solver can be
specified by the \pyth{solver} argument (in this case we are using
a conjugate gradient method, since the PDE operator is
self-adjoint). The default value for this argument is \pyth{gmres}.
More details on how to choose and customize the solvers will be given later.

\Atodo{reminder to discuss solver parameters in section 3}
\Rtodo{The scheme and the model needs to be a bit more motivated here.}
\Atodo{Any suggestions?}


Lastly we note that it is possible to explicitly define a model object
and we investigate the different ways of doing this in section \ref{sec:header}.

\subsection{Solving} \label{sec:solving}

The last natural part of a FEM is the solving, which includes
time loops, mesh refinements, data output, plotting, and so on.
Let us begin by setting up the time step $\Delta t = 0.05$.
The time step is part of the UFL equation passed to the constructor of the
\pyth{scheme}. The scheme uses that equation to construct a model with
methods to change any UFL constant found in the equation. Using an instance
of \pyth{NamedConstant} we can easily assign values to these constant using
the given name. Recall that we used
\pyth{dt = NamedConstant(space, "dt")} in our UFL equation and setting this
constant for example to \pyth{0.05} is achieved by the following code:
\renewcommand{\pweavecaption}{Setting up time variables before the
loop}
\renewcommand{\pweavelabel}{lst:forch-pre-solve}
```python, label = 'forch-pre-solve'
scheme.model.dt = 0.0025
```
Note that this does not require any additional JIT compilation. Similarly we
can use set the time in the model to zero by calling
\pyth{scheme.model.t = 0}.

Next we write the following method for solving the problem over the
time range. Since the problem is time-dependent, we
solve over a for loop with $t_0 = 0$ and $t_N = T$, using \pyth{u_h_n} for
the old solution and \pyth{u_h} for the new one.
\renewcommand{\pweavecaption}{Evolve method for solving in time}
\renewcommand{\pweavelabel}{lst:forch-evolve}
```python, label='forch-evolve'
def evolve(scheme, u_h, u_h_n, endTime):
    time = 0
    while time < (endTime - 1e-6):
        scheme.model.t = time
        u_h_n.assign(u_h)
        scheme.solve(target=u_h)
        time += scheme.model.dt
```
Note that changing \pyth{u_h_n} changes the corresponding function in the
model defined by the UFL equation given above again without having to
recompile the code.

Lastly we want to have a way of computing the error. Say for
instance we want to look at the $L^2$ and $H^1$ errors for our
computed solution, assuming an exact solution $u$ is known at the
final time of the simulation.
\begin{equation}
 L^2\text{ error} = \left( \int_{\Omega} | u - u_h |^2 \diff x
                    \right)^{\frac{1}{2}}, \qquad
 H^1\text{ error} = \left( \int_{\Omega} | \nabla(u - u_h) |^2 \diff x
                    \right^{\frac{1}{2}}.
\label{eqn:error}
\end{equation}
Computing the squared norm can be achieved with the following code.
\renewcommand{\pweavecaption}{Writing expressions for the error
computed at the final time}
\renewcommand{\pweavelabel}{lst:forch-error}
```python, label="fem6", echo=False
# <markdowncell>
# Since we have forced the system towards a given solution, we can compute
# the discretization error. First we define ufl expressions for the $L^2$
# and $H^1$ norms and will use those to compute the experimental order of
# convergence of the scheme by computing the time evolution on different grid
# levels.
# <codecell>
```
```python, label = 'error'
endTime    = 1.0
exact_end  = exact(endTime)
l2error = inner(u_h - exact_end, u_h - exact_end)
h1error = inner(grad(u_h - exact_end), grad(u_h - exact_end))
```
First of all we define the exact solution (\pyth{exact_end}) at the
end time $T=1$. Then we simply write expressions in UFL to calculate
the $L^2$ and $H^1$ errors. We note that this works even though
\pyth{u_h} is a discrete function and not a UFL term itself,
since the expression is extracted from it automatically.

We also want to compute the estimated order of convergence (EOC), to test our
method.
\begin{equation*}
  EOC = \frac{\log(e_{new}/e_{old})}{\log(h_{new}/h_{old})}.
\label{eqn:eoc}
\end{equation*}
This is calculated by refining the grid and comparing the errors
($e_{old}$ and $e_{new}$) to the grid sizes ($h_{old}$ and
$h_{new}$), where the errors are computed using the error function
\pyth{l2error} from \ref{lst:forch-error}. In particular for a grid size
that is being halved at each step, we do the following after each
solve step.
\renewcommand{\pweavecaption}{Calculating the EOCs}
\renewcommand{\pweavelabel}{lst:eoc}
\begin{pweavecode}
error_old = error                               # store old error
error = sqrt(integrate(grid, l2error, 5)[0]) # integrate
eoc = log(error/error_old)/log(0.5)             # do the EOC calc
grid.hierarchicalGrid.globalRefine(1)           # refine the grid
\end{pweavecode}
Combining these concepts into one solve method in \dune[Fempy], we have the
following program (with resulting figure \ref{fig:forch-main}).
\renewcommand{\pweavecaption}{Solving the Forchheimer equation in
time and refining the grid}
\renewcommand{\pweavelabel}{lst:forch-main}
```python, label='forch-main', fig=True, caption='Plot of solutions at each level of refinement', width='0.32\\textwidth'
from math import log
error = 0
for eocLoop in range(3):
    u_h.interpolate(initial)
    evolve(scheme, u_h, u_h_n, endTime)
    error_old = error
    error = sqrt( integrate(grid, l2error, order=5)[0] )
    if eocLoop == 0:
        eoc = '-'
    else:
        eoc = round(log(error/error_old)/log(0.5),2)
    print('step:', eocLoop, ', size:', grid.size(0),end="\t")
    print('|u_h - u| =', '{:0.5e}'.format(error), ', eoc =', eoc)
    plot(u_h)
    grid.writeVTK('forchheimer', pointdata={'u': u_h, 'l2error':
                  l2error, 'h1error': h1error}, number=eocLoop)
    if eocLoop < 2:
        grid.hierarchicalGrid.globalRefine(1)
        scheme.model.dt /= 2
```
We compile a table of the errors and EOCs for additional refinement
steps and also including the $H^1$ error below.
\Atodo{The following table is hard coded so needs to be fixed before
submission - or generated through the Makefile}
\begin{center}
  \begin{tabular}{l | l | l | l | l}
    \hline
    Elements & $\|u - u_h\|_{L^2}$ & EOC & $|u - u_h|_{H^1}$ & EOC \\ \hline
    16 & 2.919e-05 & - & 8.917e-04 & - \\
    64 & 3.611e-06 & 3.015 & 2.223e-04 & 2.000 \\
    256 & 4.500e-07 & 3.004 & 5.573e-05 & 2.000 \\
    1024 & 5.621e-08 & 3.001 & 1.393e-05 & 2.000 \\
    4096 & 7.031e-09 & 2.999 & 3.483e-06 & 2.000
  \end{tabular}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Using Alternative Linear Algebra Packages} \label{sec:solve}
```python, label="solve-methods", echo=False
# <markdowncell>
# ## Alternate Solve Methods
# Here we look at different ways of solving PDEs using external
# packages and python functionality.
# <codecell>
```
We carry on our exploration of the \dune[Fempy] feature set by
looking at the different methods of solving the PDE,
facilitated by the different storage back ends which can be chosen while
constructing the spaces. The underlying module \dune[fem] allows us to
directly store DoF vectors and system matrices in the native data
structures of a number of different linear algebra software packages. Thus
a large number of solvers and preconditioners become availbale without loss
of performance. Furthermore, for those packages providing Python bindings
for their data structures, these can be directly accessed in Python
scripts.

\Rtodo{Motivate the different storage types, basically a result of the poor
programming of the LA packages.}
\Atodo{That is a bit harsh - I tried to point out above that the
flexibility to directly use different packages is a strength of \dune[] not
a weakness of others - mach nicht immer alles schlecht :-)}

By default we use a very simple storage structure
directly provided in \dune[fem] and consequently not requiring any
additional packages. A number of simple Krylov type solvers are available
but no sophisticated preconditioning.
Changing the \pyth{storage} argument in the construction of the space makes
it possible to use more sophisticated solvers (e.g., better
preconditioners or direct solvers). Available possibilities are shown in
Table \ref{tab:discrete-functions}. In the following code the storage is
changed to use data structures provided by the module
\dune[Istl] (see \cite{Dune-Istl}) linear algebra backend.
\renewcommand{\pweavecaption}{Accessing different storage types}
\renewcommand{\pweavelabel}{lst:storage}
\begin{pweavecode} % note: code will not be executed
space = create.space('lagrange', grid, dimrange=1, order=2, storage='istl')
\end{pweavecode}

An advantage of the default \pyth{"fem"} storage is that we can
directly use solvers from SciPy \cite{SciPy}. This
allows for more complex ways of writing numerical methods without the
need to explicitly write it on the C++ side. Using the Python buffer
protocol the raw storage for the degrees of freedom and the sparse matrix
can be directly transferred to vectors from the NumPy package
\cite{NumPy} and CRS matrix from SciPy.
No copy is required and changes to the DoF vector made
on the Python side will automatically lead to changes on the C++ side.
We demonstrate this via the Forchheimer example from
Section \ref{sec:fem-in-fempy}, first by implementing a simple Newton solver:
given an initial guess $u^0$ (here taken to be zero) we solve for $n\geq 0$,
\begin{align*}
   u^{n+1} = u^n - DS^{-1}(u^n)S(u^n),
\end{align*}
We will use different linear algebra packages to invert the linearization $DS$ of the operator $S$.

Usually solving the nonlinear system is automatically taken care of in \dune[Fempy] by
\pyth{scheme.solve}, however this time we will use the call operator on the
\pyth{scheme} to compute $S(u^n)$ as well as \pyth{scheme.jacobian} to
assemble the system matrix in form of a SciPy sparse row matrix.
This can directly be done using the default storage type
\pyth{storage="fem"}. We present this
alternative below: % and plot the result in figure \ref{fig:forch-newton}.
\renewcommand{\pweavecaption}{Creating a class to hold a different
solve method}
\renewcommand{\pweavelabel}{lst:forch-newton}
```python, label="solve-methods1", echo=False
# <markdowncell>
# Implementing a simple Newton Krylov solver using the dune-fem linear solvers
# <codecell>
```
```python, label='forch-newton'
from dune.fem.operator import linear as linearOperator
import numpy as np
from scipy.sparse.linalg import cg as solver
class Scheme:
  def __init__(self, scheme):
      self.model = scheme.model
      self.jacobian = linearOperator(scheme)

  def solve(self, target=None):
      # create a copy of target for the residual
      res = target.copy(name="residual")

      # extract numpy vectors from target and res
      sol_coeff = target.as_numpy
      res_coeff = res.as_numpy

      n = 0
      while True:
          scheme(target, res)
          absF = math.sqrt( np.dot(res_coeff,res_coeff) )
          if absF < 1e-10:
              break
          scheme.jacobian(target,self.jacobian)
          sol_coeff -= solver(self.jacobian.as_numpy, res_coeff, tol=1e-10)[0]
          n += 1

scheme_cls = Scheme(scheme)

# grid.hierarchicalGrid.globalRefine(-2)  # revert grid refinement
u_h.interpolate(initial)                # reset u_h to initial
evolve(scheme_cls, u_h, u_h_n, endTime)
error = u_h - exact_end
print("size: ", grid.size(0), "L^2, H^1 error:",'{:0.5e}, {:0.5e}'.format(
  *[ sqrt(e) for e in integrate(grid,[error**2,inner(grad(error),grad(error))], order=5) ]))
```
We can redo the above computation using a Newton-Krylov solver from
SciPy. We do this by constructing a class \pyth{Df} containing the
derivative of the operator. This would normally be done within DUNE,
but here we do it purely through Python:
%, giving figure \ref{fig:forch-df} which is identical to before.
\renewcommand{\pweavecaption}{Implementing a Newton-Krylov solver with SciPy}
\renewcommand{\pweavelabel}{lst:forch-df}
```python, label="solve-methods2", echo=False
# <markdowncell>
# Using a non linear solver from the Scipy package
# <codecell>
```
```python, label='forch-df'
from scipy.optimize import newton_krylov
from scipy.sparse.linalg import LinearOperator

class Scheme2:
    def __init__(self, scheme):
        self.scheme = scheme
        self.model = scheme.model
        self.res = u_h.copy(name="residual")

    # non linear function
    def f(self, x_coeff):
        # the following converts a given numpy array
        # into a discrete function over the given space
        x = space.function("tmp", dofVector=x_coeff)
        scheme(x, self.res)
        return self.res.as_numpy

    # class for the derivative DS of S
    class Df(LinearOperator):
        def __init__(self, x_coeff):
            self.shape = (x_coeff.shape[0], x_coeff.shape[0])
            self.dtype = x_coeff.dtype
            x = space.function("tmp", dofVector=x_coeff)
            self.jacobian = linearOperator(scheme, ubar=x)
        # reassemble the matrix DF(u) given a DoF vector for u
        def update(self, x_coeff, f):
            x = space.function("tmp", dofVector=x_coeff)
            scheme.jacobian(x, self.jacobian)
        # compute DS(u)^{-1}x for a given DoF vector x
        def _matvec(self, x_coeff):
            return solver(self.jacobian.as_numpy, x_coeff, tol=1e-10)[0]

    def solve(self, target=None):
        sol_coeff = target.as_numpy
        # call the newton krylov solver from scipy
        sol_coeff[:] = newton_krylov(self.f, sol_coeff,
                    verbose=0, f_tol=1e-8,
                    inner_M=self.Df(sol_coeff))

scheme2_cls = Scheme2(scheme)
u_h.interpolate(initial)
evolve(scheme2_cls, u_h, u_h_n, endTime)
error = u_h - exact_end
print("size: ", grid.size(0), "L^2, H^1 error:",'{:0.5e}, {:0.5e}'.format(
  *[ sqrt(e) for e in integrate(grid,[error**2,inner(grad(error),grad(error))], order=5) ]))
```

We can also use solvers from the PETSc package (see \cite{PETSc}) to solve
the problem. This can be done either through bindings available in
\dune[fem] or through the \pyth{petsc4py} package
(\cite{PETSc4Py}).\footnote{For this to work, one must make sure that \dune
has been configured using the same version of PETSc used for
\pyth{petsc4py}.}

The first step is to change the storage in the space. This also
requires setting up the scheme and discrete functions again to use the
new storage structure.

We can directly use the PETSc solvers by invoking \pyth{solve} on the
scheme as before. Note that to do this we must change the storage type by
creating a new \pyth{space}. Then we have the following code, resulting in
the same approximation as computed previously:
% found once again in figure \ref{fig:forch-petsc}.
\renewcommand{\pweavecaption}{Using petsc4py to solve using PETSc}
\renewcommand{\pweavelabel}{lst:forch-petsc}
```python, label="solve-methods2", echo=False
# <markdowncell>
# Switching to a storage based on the PETSc solver package and solving the
# system using the dune-fem bindings
# <codecell>
```
```python
spacePetsc = create.space("lagrange", grid, dimrange=1, order=2, storage='petsc')
# first we will use the petsc solver available in the `dune-fem` package
# (using the sor preconditioner)
schemePetsc = create.scheme("galerkin", a == b, space=spacePetsc,
                   parameters={"petsc.preconditioning.method":"sor"})
schemePetsc.model.dt = scheme.model.dt
u_h = spacePetsc.interpolate(initial, name='u_h')
u_h_n = u_h.copy(name="previous")
evolve(schemePetsc, u_h, u_h_n, endTime)
error = u_h - exact_end
print("size: ", grid.size(0), "L^2, H^1 error:",'{:0.5e}, {:0.5e}'.format(
  *[ sqrt(e) for e in integrate(grid,[error**2,inner(grad(error),grad(error))], order=5) ]))
```
Note how we have overrode the space used to construct the UFL equation by
providing a different space to the constructor of the scheme.

Next we will implement the Newton loop in Python using \pyth{petsc4py}
to solve the linear systems.
We can access the PETSc vectors by calling \pyth{as_petsc} on the
discrete function. Note that this property will only be available if
the discrete function is an element of a space with storage
\pyth{'petsc'}. The method \pyth{jacobian} on the scheme now assembles into
a provided linear operator based on a sparse PETSc matrix and so we can directly use the KSP class
from \pyth{petsc4py}.
\renewcommand{\pweavecaption}{Using petsc4py and its Krylov solvers to
define a Newton scheme and solve}
\renewcommand{\pweavelabel}{lst:forch-petsc-ksp}
```python, label="solve-methods4", echo=False
# <markdowncell>
# Implementing a Newton Krylov solver using the binding provided by petsc4py
# <codecell>
```
```python, label='forch-petsc-ksp'
import petsc4py, sys
petsc4py.init(sys.argv)
from petsc4py import PETSc

class Scheme3:
  def __init__(self, scheme):
      self.model = scheme.model
      self.jacobian = linearOperator(scheme)
      self.ksp = PETSc.KSP()
      self.ksp.create(PETSc.COMM_WORLD)
      # use conjugate gradients method
      self.ksp.setType("cg")
      # and incomplete Cholesky
      self.ksp.getPC().setType("icc")
      self.ksp.setOperators(self.jacobian.as_petsc)
      self.ksp.setFromOptions()
  def solve(self, target=None):
      res = target.copy(name="residual")
      sol_coeff = target.as_petsc
      res_coeff = res.as_petsc
      n = 0
      while True:
          schemePetsc(target, res)
          absF = math.sqrt( res_coeff.dot(res_coeff) )
          if absF < 1e-10:
              break
          schemePetsc.jacobian(target, self.jacobian)
          self.ksp.solve(res_coeff, res_coeff)
          sol_coeff -= res_coeff
          n += 1

u_h.interpolate(initial)
scheme3_cls = Scheme3(schemePetsc)
evolve(scheme3_cls, u_h, u_h_n, endTime)
error = u_h - exact_end
print("size: ", grid.size(0), "L^2, H^1 error:",'{:0.5e}, {:0.5e}'.format(
  *[ sqrt(e) for e in integrate(grid,[error**2,inner(grad(error),grad(error))], order=5) ]))
```
Finally we we will use PETSc's nonlinear solvers (the \pyth{snes}
classes) directly.
\renewcommand{\pweavecaption}{Using petsc4py and their nonlinear
solvers (SNES) directly}
\renewcommand{\pweavelabel}{lst:forch-petsc-snes}
```python, label="solve-methods5", echo=False
# <markdowncell>
# Using the petsc4py bindings for the non linear KSP solvers from PETSc
# <codecell>
```
```python, label='forch-petsc-snes'
class Scheme4:
    def __init__(self, scheme):
        self.model = scheme.model
        self.res = scheme.space.interpolate([0],name="residual")
        self.scheme = scheme
        self.jacobian = linearOperator(self.scheme)
        self.snes = PETSc.SNES().create()
        self.snes.setFunction(self.f, self.res.as_petsc)
        self.snes.setJacobian(self.Df, self.jacobian.as_petsc, self.jacobian.as_petsc)
        self.snes.getKSP().setType("cg")
        self.snes.setFromOptions()

    def f(self, snes, X, F):
        # setup discrete function using the provide petsc vectors
        inDF = self.scheme.space.function("tmp",dofVector=X)
        outDF = self.scheme.space.function("tmp",dofVector=F)
        self.scheme(inDF,outDF)

    def Df(self, snes, x, m, b):
        inDF = self.scheme.space.function("tmp",dofVector=x)
        self.scheme.jacobian(inDF, self.jacobian)
        return PETSc.Mat.Structure.SAME_NONZERO_PATTERN

    def solve(self, target):
        sol_coeff = target.as_petsc
        self.snes.solve(None, sol_coeff)

u_h.interpolate(initial)
scheme4_cls = Scheme4(schemePetsc)
evolve(scheme4_cls, u_h, u_h_n, endTime)
error = u_h - exact_end
print("size: ", grid.size(0), "L^2, H^1 error:",'{:0.5e}, {:0.5e}'.format(
  *[ sqrt(e) for e in integrate(grid,[error**2,inner(grad(error),grad(error))], order=5) ]))
```
\noindent \textit{Remark.} The methods \pyth{as_numpy} and \pyth{as_petsc}
(returning the DoF vector or system matrix either as a \pyth{numpy} or a \pyth{petsc}
vector or matrix) do not lead to a copy of the data and the same is true for the
discrete functions returned by the \pyth{function} method on the
discrete space. In the \pyth{numpy} case Python's buffer protocol is used,
while in the case of \pyth{petsc} the
underlying C++ \pyth{Vec} and \pyth{Mat} structures can be shared using the
bindings provided by \pyth{petsc4py}. At the time of writing it
is not yet possible to pass in for example a \pyth{Mat} instance
constructed on the Python side to the \pyth{scheme.jacobian} method.
\Atodo{We should perahps add the issue with grid refinement invalidating a
previously held \pyth{matrix = jacobian.as_numpy} since the underlying
memory will have changed. Perhaps overal something on memory management,
i.e., that (now) \pyth{globaleRefine} can leed to \pyth{NaN} values for new
dofs so \pyth{clear} needs to be called or
\pyth{dune.fem.globalRefine([uh])}
should be used}

\section{Adaptive Mesh Refinement} \label{sec:crystal}

We shall now consider the implementation of adaptive mesh refinement
in \dune[Fempy]. Adaptive mesh refinement is a technique that
allows for the targeted refinement of the computational domain in
specific areas where there is greater turbulence or activity, for greater
precision. In problems where uniform refinement of a mesh is not
required, this allows for more precision of the results at less
computational cost.  

\Rtodo{What about exponential convergence rates, etc. }

The method considered here uses so-called h-adaptivity that adds
additional mesh points to the grid at areas of small scale activity.
It does so based on a marking procedure that
evaluates the gradient of the solution at each element and determines
whether to refine the grid based on a level of tolerance.

\Rtodo{Shouldn't just speak of error indicators here?}

In this section we present two examples which use adaptive grid
refinement in slightly different ways.
\Atodo{add something on the new \pyth{dune.fem.mark} in the notebook}

\input{laplace-adaptive.tex}

We shall now look at a model for crystallization on the surface of a
liquid.

\input{crystal.tex}
\section{Moving Meshes} \label{sec:mcf}

In this section we will consider an example where the grid itself
changes over time subject to PDEs. Specifically we refer to a
\textit{geometric evolution equation}, which describes the motion of a
hypersurface by prescribing its velocity geometrically. In
\dune[Fempy], it is possible to accomplish this through the following
process
\begin{enumerate}
\item Create an interpolated function that describes the initial
surface, i.e.
\begin{lstlisting}[style=pythonstyle]
positions = space.interpolate(lambda x: some_function(x), name="position")
\end{lstlisting}
\item Create a surface from \pyth{positions} using this function
\begin{lstlisting}[style=pythonstyle]
surface = create.view("geometry", positions)
\end{lstlisting}
\item Create the scheme that describes the surface evolution and solve
it in the usual way.
\item Update the surface using the computed solution.
\begin{lstlisting}[style=pythonstyle]
positions.dofVector.assign(solution.dofVector)
\end{lstlisting}
\end{enumerate}
With this process the surface (and by extension the mesh) can be
changed over time. We will now demonstrate this in a mean curvature flow
example.

\subsection{Mean Curvature Flow}

Mean curvature flow is a specific example of a geometric evolution
equation where the evolution is governed by the mean curvature $H$.
One real-life example of this is in how soap films change over time,
although it can also be applied to image processing (e.g.
\cite{Malladi}).
Assume we can define a reference surface $\Gamma_0$ such that
we can write the evolving surface $\Gamma(t)$ in the form
\begin{gather*}
  \Gamma(t) = X(t,\Gamma_0).
\end{gather*}
It is now possible to show that the vector valued function $X=X(t,x)$
with $x\in\Gamma_0$ satisfies
\begin{gather*}
  \frac{\partial}{\partial_t}X = - H(X)\nu(X),
\end{gather*}
where $H$ is the mean curvature of $\Gamma_t$ and $\nu$ is its outward pointing normal.

We will solve this using a finite element approach based on the
following time discrete approximation from \cite[Eqn 4.16]{Charlie}
(with a $\theta$-scheme applied).
\begin{gather*}
  \int_{\Gamma^n} \big( U^{n+1} - {\rm id}\big) \cdot \varphi +
    \tau \int_{\Gamma^n} \big(
    \theta\nabla_{\Gamma^n} U^{n+1} + (1-\theta) I \big)
    \colon\nabla_{\Gamma^n}\varphi
  =0.
\end{gather*}
Here $U^n$ parametrizes $\Gamma(t^{n+1})$ over
$\Gamma^n:=\Gamma(t^{n})$, $I$ is the identity matrix, $\tau$ is the
time step and $\theta\in[0,1]$ is a discretization parameter.

\input{mcf.tex}

\begin{comment}
\section{Partitioned Grids} \label{sec:battery}

As another application of grid techniques, we look at a problem where
 we want to divide the grid into three regions. We do this using an
\pyth{'adaptive'} grid that allows for grid filters to be applied. We
note that another way of creating multi-domain grids in \dune is
described in \cite{Muthing} (though \dune[Fempy] bindings are not yet
available).

\input{battery.tex}
\end{comment}

\section{Translating Python Code to C++} \label{sec:mcf-comparison}

Having looked at some of the functionality available for solving
various FEM problems, we now shift our focus towards more in-depth
features that concern efficiency and C++ development. Here we consider
the idea of moving sections of Python code
over to C++ for efficiency. A key aspect of the design of \dune[Fempy] has
been about keeping the structure of the C++ code in the Python code's
design, to the point where translating between the two is relatively painless. In
particular this allows for rapid prototyping of methods in Python with
its relative ease of use, after which code can be ported to C++ for
efficiency if necessary in large-scale computation.

Here we will demonstrate this translation process, and additionally
provide comparisons for the difference in efficiency timewise. We will
examine the function used for calculating the averaged radius of a
surface used in section \ref{sec:mcf}.
\renewcommand{\pweavecaption}{A pythonic function for calculating the
radius of a surface}
\renewcommand{\pweavelabel}{lst:calcR}
\begin{pweavecode}
def calcRadius(surface):
    # compute R = int_x |x| / int_x 1
    R   = 0
    vol = 0
    for e in surface.elements:
        rule = geometry.quadratureRule(e.type, 4)
        for p in rule:
            geo = e.geometry
            weight = geo.volume * p.weight
            R   += geo.toGlobal(p.position).two_norm * weight
            vol += weight
    return R/vol
\end{pweavecode}
As a relatively simple example, this code is not particularly slow in
python, however the existence of callbacks inside a looped statement
are not insignificant. Now let us look at a C++ translation of the above
code.
\renewcommand{\pweavecaption}{The C++ version of the calcRadius
function}
\renewcommand{\pweavelabel}{lst:calcR-cpp}
\begin{pweavecode}
#include <dune/geometry/quadraturerules.hh>

template< class Surface >
double calcRadius( const Surface &surface )
{
  double R = 0.;
  double vol = 0.;
  for( const auto &entity : elements( surface ) )
  {
    const auto& rule = Dune::QuadratureRules<double, 2>::rule(entity.type(), 4);
    for ( const auto &p : rule )
    {
      const auto geo = entity.geometry();
      const double weight = geo.volume() * p.weight();
      R   += geo.global(p.position()).two_norm() * weight;
      vol += weight;
    }
  }
  return R/vol;
}
\end{pweavecode}
We note that we take advantage of C++11 features such as \cpp{auto}
and range based for loops to keep a similar structure to the Python
code.

Supposing we save the above as \cpp{radius.hh}, we can then call it
in a Python script and use it like a regular function as follows.
\renewcommand{\pweavecaption}{Calling our C++ function using algorithm}
\renewcommand{\pweavelabel}{lst:calcR-algorithm}
\begin{pweavecode}
from dune.generator import algorithm
calcRadius = algorithm.load('calcRadius', 'radius.hh', surface)
\end{pweavecode}
Doing this, we can quite easily swap between the two versions and
compare the runtime of the solve method. We show these results in
figure \ref{fig:mcf-comparison}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth, height=8cm,
keepaspectratio]{figures/mcf-comparison.png}
\caption{Comparison of time taken between the two calcRadius methods}
\label{fig:mcf-comparison}
\end{figure}

What we see is that the C++ version is roughly 18\% faster. On a small
scale this is not a significant change, but it shows that it can be
worth the extra coding time for longer running simulations. Naturally
a full C++ version would see a further improvement in runtime, though
whether that is justified depends on a case-by-case basis.

\section{Model C++ Header Generation} \label{sec:header}

After having looked at the basics of the Python
interface, let us now consider features more aimed at C++
integration and code extensibility.

In section \ref{sec:schemes}, we briefly alluded to the fact that the UFL
equation passed to the \pyth{scheme} constructor is used to create a
\pyth{model} class. To this end a C++ class is generated and JIT
compilation is used to export this class to Python.
We can separate the process into two steps as follows.
\renewcommand{\pweavecaption}{A simple integrands model}
\renewcommand{\pweavelabel}{lst:integrands}
\begin{pweavecode}
model = create.model('integrands', grid, a == b)
scheme = create.scheme("galerkin", model)
\end{pweavecode}
where \pyth{a == b} refers to the UFL equation used to represent weak form
of the PDE, i.e., the bilinear and linear form,
and \pyth{'integrands'} refers to form of C++ class used to represent
second order operators in \dune[Fem]. A form of this C++ representation
that is more human readable is given by
\renewcommand{\pweavecaption}{A simple elliptic model}
\renewcommand{\pweavelabel}{lst:elliptic}
\begin{pweavecode}
model = create.model('elliptic', grid, a == b)
scheme = create.scheme("h1", model)
\end{pweavecode}
While the generated class is easier to read and change it is less flexible
then the \pyth{integrands} model, e.g., interior facet integrals can not be
directly represented in this class.

We note that this is a different approach to similar packages, like Fenics
\cite{FEniCS} and Firedrake \cite{Firedrake}, which
in general do not create a human readable C++ class that only depends on
the PDE itself and not on discretization details, e.g., the discrete spaces
or quadratures used. We are planning to adding this approach in the future
to be able to generate more efficient code in some special cases.

\subsection{Elliptic Models}

First let us consider the simpler \pyth{elliptic} model. In mathematical
terms, consider the same general operator we defined previously.
\begin{equation}
  L[u] = -\nabla \cdot D(x, u) \nabla u + m(x, u, \nabla u).
  \tag{\ref{eqn:elliptic}}
\end{equation}
In variational form, after multiplying with a test function and
integration by parts (ignoring boundary terms for now), we arrive at
\[
  L[u] = \int_{\Omega}D(x, u) \nabla u \cdot \nabla v + m(x, u, \nabla u)
  v \diff x.
\]
Possibly with an additional boundary integral representing non trivial
boundary conditions.
Now the elliptic model class in \dune[Fempy] has methods that represent
the above form in general terms. Most importantly it has four methods, two
to compute $D(x, u) \nabla u$ and the corresponding linearization
and a second pair to compute $m(x, u, \nabla u)$ and its linearization.
Additionally there are
methods for the associated Dirichlet or Neumann boundary conditions.
Together these form the \pyth{elliptic} model class templated over the underlying
type of the grid. As mention above this is only
one way of expressing bilinear forms in \dune[Fempy].
% Generally this
% class is then used to create a shared object file that is exported to
% python using pybind11 for use in python scripts and notebooks.

As an example assume for instance we were to take
the case of $m(x, u, \nabla u) = u$ above. In the model class, this would be
defined under the method \cpp{source}.
\renewcommand{\pweavecaption}{A function in the elliptic model C++
class}
\renewcommand{\pweavelabel}{lst:elliptic-cpp}
\begin{pweavecode}
template< class Point >
void source ( const Point &x, const RangeType &u, const JacobianRangeType &du, RangeType &result ) const
{
  result = u;
}
\end{pweavecode}

\Rtodo{I would rather write result = u, then it holds for all dimRangei.}
\Atodo{I think that is not what is generated - bit complicated to figure
out that special case in the code generation process}

Another possibility provided for by the modular design of
\dune[python] and the simple structure of the class representing the
elliptic model, is the creation of alternate
models. This approach involves the writing of additional C++
classes (one example would be an elliptic discontinuous Galerkin model) based on
the elliptic model class except with extra modifications that one
might want to make to the underlying structure.
Whilst this approach is more in-depth than simply editing
an elliptic header file, it allows one to change the functions
themselves beyond what the default elliptic model accepts.
\Atodo{the above is not too clear, i.e., what is actually needed?}

\Atodo{Do we keep the following or just chite Lloyd's thesis a future paper?
We could even consider leaving the elliptic model out of the paper and only
show how to use the integrands model in a C++ code...}
An example of this approach is the \textit{nonvariational} model
for the \dune[femnv] module (\cite{NVPAPER}). This comes
from the desire to write bilinear forms that can accept a Hessian as
as argument as follows.
\begin{equation*}
  L[u] = \nabla \cdot D(x, u) \nabla \varphi(x) + m(x, u, \nabla
  u, D^2 u)\varphi.
\end{equation*}
Such a change would require different arguments to be made available
to the methods from the elliptic model. Suppose we wanted to
implement the nonvariational Poisson equation, i.e. taking $m = -\Delta u$
above. Then we would need the following method.
\renewcommand{\pweavecaption}{A nonvariational method}
\renewcommand{\pweavelabel}{lst:nvop}
\begin{pweavecode}
template< class Point >
void source ( const Point &x, const RangeType &u, const
JacobianRangeType &du, const HessianRangeType &d2u, RangeType &result ) const
{
  result[ 0 ] = d2u[ 0 ][ 0 ] + d2u[ 1 ][ 1 ];
}
\end{pweavecode}
Whilst this may not be immediately possible with the standard elliptic
model, it is possible to create a model \cpp{'nvdg'} that can
use such functions, which results in the ability to write functional
\dune[Fempy] code as follows.
\renewcommand{\pweavecaption}{The \dune[Fempy] code for a nonvariational
model}
\renewcommand{\pweavelabel}{lst:fempy_nv}
\begin{pweavecode}
a = -(grad(grad(u[0])[0])[0] + grad(grad(u[0])[1])[1])*v[0]*dx
b = rhs(A, exact)
model = create.model("nvdg", grid, a == b)
\end{pweavecode}
Thus it becomes possible to write schemes that expect different
arguments from the operator.

\Rtodo{Here I would again explain a bit what is possible with this approach and
what couldn't be done with Firedrake and Fenics. One has to be careful though. }


\subsection{Integrands Models}

We note however that as mentioned before, there exists another way of
constructing operators, by using \pyth{'integrands'}.
% This method
% bypasses the virtual methods used in the elliptic operator class and
% creates methods purely using the UFL expressions given to it.
This allows for a more general description of second order operators not
expressible by the \pyth{elliptic} model class. For example we can express
a non variational second order problem directly:
\renewcommand{\pweavecaption}{Usage of integrands operators for
skeleton terms}
\renewcommand{\pweavelabel}{lst:fempy_integrands}
\begin{pweavecode}
a = -(grad(grad(u[0])[0])[0] + grad(grad(u[0])[1])[1])*v[0]*dx
    + jump(A*grad(u[0]), n)*avg(v[0])*dS
b = rhs(A, exact)
scheme = create.model("integrands", space, a == b)
\end{pweavecode}
In addition as the above shows we can add a term
defined only on the skeleton (the facets) of the mesh. Due to the
ability to add such interior terms, the \pyth{integrands} class is
for example useful to construct discontinuous Galerkin methods.

\subsection{C++ Models} \label{sec:cpp-models}

In addition to the automatic creation of a shared library object that
is done when the model is imported into Python, it is
possible to generate a stand alone header file containing the model class
That is, it is possible to generate a C++ file (e.g. \cpp{'model.hh'}) that can be
used flexibly in both \dune[Fempy] and regular C++ compatible \dune code.
Within a \dune[] module depending on \dune[Fempy], we can do this by
writing a pure UFL file and calling cmake on it.

Let us examine what this file looks like for the Forchheimer model:
\renewcommand{\pweavecaption}{UFL file used for C++ header file generation}
\renewcommand{\pweavelabel}{lst:header}
\begin{pweavecode}
from dune.fem import Space
space = Space(2, 1)
u = TrialFunction(space)
v = TestFunction(space)
x = SpatialCoordinate(space.cell())
dt = NamedConstant(triangle, "dt")    # time step
t  = NamedConstant(triangle, "t")     # current time
u_h_n = NamedCoefficient(space,"previous")

initial = 1/2*(x[0]**2 + x[1]**2) - 1/3*(x[0]**3 - x[1]**3) + 1
abs_du = sqrt(inner(grad(u), grad(u)))
K = 2/(1 + sqrt(1 + 4*abs_du))
a = (inner((u - u_h_n)/dt, v) + inner(K*grad(u), grad(v)))*dx
exact = as_vector( [exp(-2*t)*(initial - 1) + 1] )
b = replace(a, {u: exact})

F = a - b
\end{pweavecode}
Once the corresponding \cpp{forchheimer.hh} file has been generated,
it can then be edited manually, and then used in place
of a UFL expression in \dune[Fempy]. This choice of default shared library
generation or usable header files falls in line with
attempts we have made to improve extensibility of the code, since in
particular it allows for the user to write in more complex features in
C++ that do not necessarily have Python bindings written for them. An
additional advantage is that this header file can be directly used within a
traditional \dune[] C++ program.

What the above allows us to do is to easily investigate,
the difference in efficiency between using the
full \dune[Fempy] interface to run problems and writing a pure C++ version
to solve the same problem. The model file needed to run the C++ \dune[Fem]
code can be generated using a UFL expression as explained above.
To look at this in more detail we constructed
an identical Forchheimer example in C++ which can be found in section
\ref{sec:forch-cpp}. We compare the runtime of the two solve-steps
below.
\begin{center}
  \captionof{table}{Runtimes for Forchheimer solve time}
  \begin{tabular}{l | l}
    \hline
     & Runtime (s) \\ \hline
    C++ & 78.3 \\
    Python & 83.0
  \end{tabular}
\end{center}
Thus we see there is not a sizable difference between the \dune[Fem] and
\dune[Fempy] versions.

\Rtodo{When I tested this the run times were the same. So double check please.}

\noindent \textit{Remark.} We remark that both versions use preprocessing, which relatively
speaking is negligible for long-running simulations. We also note that
the most \textit{costly} aspect of pure Python code are generally
callbacks, and in this example this amounts to just the solve call. An
additional example that taxes the two versions differently can be
found in section \ref{sec:mcf-comparison}.

\section{Virtualization} \label{sec:virtualization}

One final topic we want to discuss is virtualization and the
rationalities behind various design decisions. In the development of the
python bindings for \dune[Python], one decision that was made was to
avoid introducing a virtual layer when exporting classes from C++ to Python or
vice versa. Such a layer would introduce additional code maintenance
and more importantly perhaps lead to loss in performance when a C++ object
is passed through Python back into C++. In this case code optimization
steps like inlining or loop unrolling could not be utilized to their
full potential.

Take as an example a discrete function which is constructed
using \pyth{df = space.interpolate([0], name="df")}. The call to
\pyth{interpolate} goes back to the corresponding function in \dune[fem] and
returns an instance of the discrete function. To store the solution to a PDE
problem in \pyth{df}, the \pyth{solve} method on a \pyth{scheme} is
called. While executing \pyth{scheme.solve(target=df)} the discrete
function instance is passed back to another \dune[fem] function. If \pyth{df}
were virtualized (i.e. type erased) in either of the two steps, i.e. when
passed to or from Python, then the \pyth{solve} method could not work with
the same efficiency as when used in a pure C++ environment. The number of
degrees of freedom, local structure, etc. of the discrete function would
only be known as dynamic properties, making code optimization by the compiler
or the \dune developer implementing the \pyth{solve} method more difficult
or even impossible. Note that virtualizing the discrete function for
example, would almost certainly also require virtualization of the
underlying discrete function spaces (with mapper and basis function set),
and the underlying grid view (with its iterators). The cumulative effect
of this would be a quite severe hit on performance.

To avoid this issue, no type erasure is carried out
when an object is passed into Python. So in the above example the call to
\pyth{interpolate} returns an object which still contains the full type
information of the underlying \dune[fem] function. This approach leads to
compilation overhead the first time a new type of discrete function is used
since a new Python module needs to be generated. But this overhead occurs only
the first time the discrete function is used during the development of a
project and is thus negligible. Since no type erasure has occurred,
any \dune object can now be passed back to it. The \pyth{solve} method on the scheme is
exported in such a way that the target argument has to be of the same discrete
function type that was defined by the \pyth{storage} argument provided during the
space construction. Consequently a scheme over a given space (e.g. a
Lagrange space of a fixed order using an \pyth{istl} storage)
will only accept one type of discrete function as \pyth{target} argument
for its solve method. As described before the advantage of this is that the
full static type information is available at the cost of more compilation
requirements when changes (e.g. to the storage back end) are made.

There are a few exceptions to the above rule, where Python objects
passed as arguments to C++ functions undergo type erasure
if their type does not match the exact type of the arguments of that
function.

An example is the \pyth{__call__} method on an \pyth{operator}.
When calling \pyth{op(arg,dest)}, the destination parameter
(\pyth{dest}) has to be of the correct discrete function type, but for
the argument parameter (\pyth{arg}) it can make
sense to allow for a wide range of grid functions, e.g. an exact solution
given by a UFL expression or a different type of discrete function. In many
cases the implementation of the operator does not require the argument to even
be discrete since only the evaluation of \pyth{arg} at quadrature points is
required; in this case any grid function is a valid argument.
On the C++ side the operator call is simply implemented as a template
method on the operator class with the signature
\begin{lstlisting}[style=cppstyle]
template <class GF> Operator::operator()(const GF &arg, typename Operator::DiscreteFunction &dest);
\end{lstlisting}
We note that it is not possible to export a template method to Python without
fixing all of its arguments. Since an optimized version of such a method
is often implemented for the case that \pyth{arg} is of the same type as
\pyth{dest}, the default method that will always be exported to Python
has \cpp{GF=Operator::DiscreteFunction}. In addition, a second version is
exported where \cpp{GF=VirtualizedGridFunction<...>}, which is a
type erased implementation of a grid function. Any grid function exported
to Python (e.g. UFL expressions, discrete function etc.) will implicitly
convert to a \cpp{VirtualizedGridFunction} so that \pyth{op(arg,dest)} can
be used in Python even in the case that \pyth{arg} is not of the same type
as \cpp{dest}. Optimal code is still produced in the case where both
parameters are of the same type.

A second use of type erasure where objects are passed back to C++ occurs
when an \pyth{operator} or \pyth{scheme} is constructed from a given model.
Since the development of a new model can involve repeated changes
being made to it (e.g. its underlying UFL form) we aimed to avoid the
situation of each change requiring a recompilation of the \pyth{operator} or
\pyth{scheme}. To this end the model is virtualized when it is passed to
the constructor of the \pyth{operator} or \pyth{scheme} class.
Consequently, these classes only depend on some type information like the
underlying type of the grid view and the range dimension of the model but
not on the actual details of the bilinear form. The consequence of this
approach is that evaluating some part of the form introduces a virtual
function call. At the same time UFL coefficients are stored as type erased
\pyth{VirtualizedGridFunction} in the model class again to avoid too much
recompilation when changes to the UFL equations are made.
\Atodo{
explain that tests have shown that this is negligible and show some
performance test. Mention virtualization parameter to
disable type erasure if required.
}
\Atodo{We should add some information on typical compilation times}


%</tag>

\bibliographystyle{abbrvnat}
\bibliography{dune-fempy}

\begin{appendix}

%<*append>

\section{Installation}
\label{sec:installation}

\Atodo{Taken from dune-python paper - need to adapt}

The simplest approach for testing \dune[Python] is to use the provided
Docker image. It includes a number of Jupyter notebooks showcasing
the possibilities of the \dune Python bindings. There is a special Docker
image accompanying this paper. It can be used by executing
\begin{bash}
docker run --rm -v dune-python-paper:/dune -p 127.0.0.1:8888:8888 registry.dune-project.org/staging/dune-python:paper2018
\end{bash}
The Jupyter server can then be accessed from a web browser at \url{http://127.0.0.1:8888};
the password is \file{dune}.
The code examples from this paper are included in the notebook file
\file{paper2018.ipynb}.
Notice that this method works on Linux, MacOS, and Windows alike, although it
might be necessary to increase the amount of system memory given to Docker to
4~GB, e.g., on MacOS.

\Atodo{mention the \pyth{dune.create.get} method - good name to be found, perhaps
       \pyth{inspect,help,show}...}

If you already have the \dune core modules either installed or in local
space, it suffices to download the \dune[Python] module
\begin{bash}
export GITURL=https://gitlab.dune-project.org/staging/dune-python
wget -qO - ${GITURL}/repository/archive.tar.gz?ref=releases/2.6 | tar xz
\end{bash}
and to configure the module by running \code{dunecontrol}. Then
set the environment variable \code{PYTHONPATH} to include the \code{python}
subfolder in the build directory of \dune[Python], e.g.,
\code{dune-python/build-cmake/python}.
% The code used for the paper is
% available as Python script and as Jupyter notebook in the `paper2018`
% folder in the \dune[Python] source directory.

For some of the example \dune[ALUGrid] and for the final example
\dune[Polygongrid] will be required in addition to
the core modules. After building these modules the corresponding `python`
subfolder in the build directories of these modules also need to be added
to \code{PYTHONPATH}.
For a more permanent installation of \dune[Python] we suggest to set up a
virtual environment and install all necessary \dune modules into it.
More details can be found in the \url{README.md} file or on the main page of
the GitLab repository of \dune[Python].

\begin{comment}
\section{Derivation of Forchheimer Model} \label{sec:forch}
The origin of this equation stems from Darcy's law, an equation that
describes flow through porous media, and is applied regularly to
groundwater flow models.
\begin{equation*}
  -\nabla p = \frac{\mu}{\kappa} v,
\label{eqn:darcy}
\end{equation*}
where $p, v, \mu$ and $\kappa$ are the pressure, velocity, absolute
viscosity and permeability. For situations where the Reynolds number
is greater than $\sim 10$, inertia begins to have an effect on the system,
which is accounted for in the Darcy-Forchheimer equation. In its most
general form we have the following.
\begin{equation*}
  -\nabla p = \sum^N_{i=0}a_i |v|^{\alpha_i}v,
\label{eqn:darcy-forch}
\end{equation*}
where $a_i$ and $\alpha_i$ are obtained empirically. Through some
manipulations, we can simplify this to an equation for just the
pressure $\rho$.
\begin{equation*}
  \rho_t - \nabla \cdot (K(|\nabla \rho|) \nabla \rho) = f,
\label{eqn:forch}
\end{equation*}
where the function $K: \mathbb{R}^{+} \to \mathbb{R}^{+}$ is dependent
on the $a_i$ and $\alpha_i$ above. Adding in boundary data and initial
values gives us the \textbf{boundary value problem}.
\begin{equation*}
  \begin{array}{ll}
  \rho_t - \nabla \cdot (K(|\nabla \rho|) \nabla \rho) = f & \mbox{in
  } \Omega \times [0, T], \\
  \rho(x, 0) = \rho^{0}(x) & \mbox{in } \Omega, \\
  K(|\nabla \rho|) \nabla \rho \cdot \textbf{n} + g(x) = 0 & \mbox{on }
  \Gamma \times [0, T],
  \end{array}
\label{eqn:forch-bvp}
\end{equation*}
where $\rho^0$ and $g$ are initial and boundary data given. Thus
the \textbf{weak form} or variational formulation follows.
\begin{equation*}
  (\rho_t, \varphi_h) + (K(|\nabla \rho|) \nabla \rho, \nabla \varphi_h) = - <
  g, \varphi_h> + (f, \varphi_h), \quad \varphi_h \in V_h,
\label{eqn:forch-weak}
\end{equation*}
with $\rho(x, 0) = \rho^{0}(x)$. Finally it remains to discretize the
equation in time using a backward Euler method. Let the time domain $I
= [0,T]$ be divided into $N$ intervals $t_0 = 0 < t_1 < \dots < t_N = T$
such that $\Delta t = t_n - t_{n-1}$ and $\rho^{n} = \rho(x, t_n)$.
Then we have the time discretized PDE.
\begin{equation*}
  \left(\frac{\rho^{n+1} - \rho^{n}}{\Delta t}, \varphi_h \right) +
  (K(|\nabla \rho^{n+1}|) \nabla \rho^{n+1},
  \nabla \varphi_h) = - < g, \varphi_h > + (f, \varphi_h), \quad
  \varphi_h \in V_h.
\label{eqn:forch-time}
\end{equation*}
We note that in the form given in (\ref{eqn:forch-roche}), $g = 0$ and
we replace $\rho$ with $u$ for simplicity.
\end{comment}

\section{C++ Version of Forchheimer Example} \label{sec:forch-cpp}

Here we present the C++ version of the Forchheimer example from
section \ref{sec:fem-in-fempy} using \dune[FEM], that we compare to
the Python version in section \ref{sec:cpp-models}.

\inputcpp{./forchheimer/main.cc}{}{lst:forch-cpp}

\section{List of Dune-Python modules} \label{sec:modules}

Here we list the different modules that are available to \dune[Python]
and \dune[Fempy] at the time of writing. We will divide them by component
into different sections that reflect the structure we have previously
introduced.

\subsection{Grids} \label{sec:grid-list}

Grids by default take the following form.
\renewcommand{\pweavecaption}{The default form for grid creation}
\renewcommand{\pweavelabel}{lst:default-grid}
\begin{pweavecode}
grid = create.grid('class-name', constructor, dimgrid=None, dimworld=None)
\end{pweavecode}
Where in particular we have the following arguments.

\begin{enumerate}
\item \pyth{'class-name'}: One of the strings from the table below.
\item \pyth{constructor}: Either a dgf \textit{dune grid format} file, a
gmesh file, or a preset object similar to what is demonstrated in
section \ref{sec:grids}.
\item \pyth{dimgrid} (optional): The dimension of the grid.
\item \pyth{dimworld} (optional): The dimension of the space the grid is in.
\end{enumerate}
The dimensions of the grid do not have to be passed to the constructor if
they can be determined from the \pyth{constructor} argument.

The table below shows a list of possible grid implementations for which
binding are available at the time of writing.

\begin{center}
  \captionof{table}{Grids}
  \begin{tabular}{l | l | l}
    \hline
    Class & Module & Description \\ \hline \hline
    \input{tables/features_grid}
  \end{tabular}
\end{center}

Grids can also be constructed to provide a different grid view:
\renewcommand{\pweavecaption}{Creating a custom grid view}
\renewcommand{\pweavelabel}{lst:default-view}
\begin{pweavecode}
view = create.view('class-name', grid)
\end{pweavecode}

The first argument can be any of the ones allowed for the grid construction
(in which case the same \pyth{LeafGridView} is constructor as when directly
constructing the grid). In addition the following views can be
constructed.
\begin{center}
  \captionof{table}{Gridviews}
  \begin{tabular}{l | l | l}
    \hline
    Class & Module & Description \\ \hline \hline
    \input{tables/features_view}
  \end{tabular}
\end{center}

\subsection{Spaces} \label{sec:space-list}

Now we move on to spaces. By default, they take the form
\renewcommand{\pweavecaption}{The default form for space creation}
\renewcommand{\pweavelabel}{lst:default-space}
\begin{pweavecode}
space = create.space('class-name', grid, dimrange=1, order=1,
                     storage='fem', field='double')
\end{pweavecode}
where we have the following arguments.

\begin{enumerate}
\item \pyth{'class-name'}: A string from the table below.
\item \pyth{grid}: A grid object from above.
\item \pyth{dimrange} (optional): The range dimension of the space.
\item \pyth{order} (optional): The polynomial order of the basis
functions.
\item \pyth{storage} (optional): \pyth{'fem'} by default. Specifies
the storage used for discrete functions (as shown in
\ref{tab:discrete-functions}.
\item \pyth{field} (optional): The field of the range space
(\pyth{'double'} or \pyth{'complex'}).
\ref{tab:discrete-functions}).
\end{enumerate}

\begin{center}
  \captionof{table}{Discrete Functions}
  \label{tab:discrete-functions}
  \begin{tabular}{l | l | l}
    \hline
    Class & Module & Description \\ \hline \hline
    \input{tables/features_discretefunction}
  \end{tabular}
\end{center}
Available spaces are:
\begin{center}
  \captionof{table}{Spaces}
  \begin{tabular}{l | l | l}
    \hline
    Class & Module & Description \\ \hline \hline
    \input{tables/features_space}
  \end{tabular}
\end{center}

\subsection{Grid Function}

Grid functions can be constructed in a variety of ways, though the explicit
way to make a grid function is
\renewcommand{\pweavecaption}{The default form for function creation}
\renewcommand{\pweavelabel}{lst:default-function}
\begin{pweavecode}
create.function('class-name', grid, 'function-name', order,
                constructor)
\end{pweavecode}
with the following arguments.
\begin{enumerate}
\item \pyth{'class-name'}: A string from the table below.
\item \pyth{grid}: A grid object from \ref{sec:grid-list}.
\item \pyth{'function-name'}: A string for the name of the created
function.
\item \pyth{order}: The order of the function used for quadrature.
\item \pyth{constructor}: An object described in the table below used
to construct the function.
\end{enumerate}
For the below table we list the available strings for
\pyth{'class-name'} together with the compatible argument for the
\pyth{constructor} parameter.
\begin{center}
  \captionof{table}{Grid Functions}
  \begin{tabular}{l | l | l }
    \hline
    Class & Module & Constructor \\ \hline \hline
    \input{tables/features_function}
  \end{tabular}
\end{center}
In addition \pyth{'discrete'} can be used to construct discrete functions.
But typically these are made using the
\pyth{space.interpolate(expression)} syntax, using the storage type
for the space by default. The arguments for the \pyth{interpolate}
method are the same used to construct general grid functions given in
the previous table.

\subsection{Schemes and Operators} \label{sec:scheme-list}

As discussed in section \ref{sec:schemes}, schemes can be constructed
directly with a UFL equation and contain the method for solving the PDE.
% Addtionally as shown in section \ref{sec:header} it is also possible to create
% models and operators that store the operator separately.
In addition UFL forms can be used to construct operators which do not have 
While schemes have to have identical domain and range spaces, operators can
map between different spaces.
We review this below.
\renewcommand{\pweavecaption}{The default form for scheme creation}
\renewcommand{\pweavelabel}{lst:default-scheme}
\begin{pweavecode}
scheme = create.scheme('class-name', equation, space,
                       parameters=dict, solver='solver-name')
\end{pweavecode}
\begin{enumerate}
\item \pyth{'class name'}: A string from the table below.
\item \pyth{equation}: A UFL equation (\pyth{a == b}), or a model
object. In addition a tuple or list can be used here
where the first entry is the equation and further arguments can provide
Dirichlet boundary conditions.
\item \pyth{space}: A space object from \ref{sec:space-list}. (This is
optional if the trial/test UFL functions are initialized with a
\dune[Fempy] discrete function space, as in code listing
\ref{lst:forch-variables}).
\item \pyth{parameters} (optional): A dictionary of \dune parameters that can be used
to specify things like the solver behaviour, e.g.
\pyth|{'newton.tolerance': 1e-3}| a more detailed description of these
parameters will be given later.
\item \pyth{solver} (optional): \pyth{'fem'} by default. Used to
specify the solver used, from \ref{tab:solvers}.
\end{enumerate}
The two main schemes available in \dune[fempy] are
\pyth{galerkin} and \pyth{h1}. The distinction between them is
described in section \ref{sec:header}. In addition the \dune[nvdg]
module provides an \pyth{nvdg} scheme.

There are a number of parameters that can be passed to the scheme to
influence the solving procedure. Most importantly tolerances for the
iterative solvers can be provided and verbosity can be turned on and off.
In addition preconditioners can be set via the parameters.

\Atodo{discuss the scheme parameters at the end of section describing the
backends or somewhere else?}
\begin{comment}
Available options depend on the
\pyth{storage} backend used to construct the space.
For the \pyth{istl} backend available options
include: \pyth{none, ssor, sor, ilu-0, ilu-n, gauss-seidel, jacobi}
and \pyth{amg-ilu-0}. They are set using the following syntax.
\renewcommand{\pweavecaption}{How preconditioning is set via
parameters}
\renewcommand{\pweavelabel}{lst:preconditioning}
\begin{pweavecode}
from dune.fem import parameter
parameter.append({"istl.preconditioning.method": "ilu",
                  "istl.preconditioning.iterations": 1,
                  "istl.preconditioning.relaxation": 1.2})
\end{pweavecode}
For the \pyth{petsc} backend options include:
\pyth{none}, \pyth{oas}, \pyth{sor}, \pyth{jacobi}, \pyth{hypre},
\pyth{ml}, \pyth{ilu}, \pyth{icc}, and \pyth{lu}. Which of these
can actually be used will depend on the PETSc implementation, e.g.
\pyth{hypre} requires that PETSc was build with support for the
\pyth{hypre} package.
\end{comment}

Finally we list the possible solving methods available that can be
selected during scheme creation above.
\begin{table}[ht!]
  \centering
  \caption{Solvers}
  \label{tab:solvers}
  \begin{tabular}{l | l}
    \hline
    Class & Description\\ \hline \hline
    \input{tables/features_solver}
  \end{tabular}
\end{table}
There is also the possibility of constructing an operator that can be
applied using the \pyth{__call__} method and linearised using the
\pyth{jacobian} method but does not provide a \pyth{solve} method.
This is especially of interest in the case where the operator maps
between different spaces. Creating an operator is similar to the scheme
construction.
\renewcommand{\pweavecaption}{The default form for operator creation}
\renewcommand{\pweavelabel}{lst:default-operator}
\begin{pweavecode}
scheme = create.operator('class-name', equation, domainSpace,
                         rangeSpace)
\end{pweavecode}

%</append>

\end{appendix}

\end{document}
